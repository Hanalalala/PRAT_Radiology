{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "default_folder = r''  # 设置默认文件夹路径\n",
    "\n",
    "# 更改当前工作目录为默认文件夹路径\n",
    "os.chdir(default_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import plot_roc_curve,roc_curve,auc,roc_auc_score\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "#from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "tumor = pd.read_excel(\"\")\n",
    "vat6 = pd.read_excel(\"\")\n",
    "all = pd.read_excel(\"\")\n",
    "tumor_1 = pd.read_excel(\"\")\n",
    "vat6_1 =pd.read_excel(\"\")\n",
    "all_1 = pd.read_excel(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# Assuming you have defined y_true1, y_prob1, y_true2, y_prob2, ..., y_true6, y_prob6\n",
    "# y_trueX should contain the true labels, and y_probX should contain the predicted probabilities for each model and dataset\n",
    "\n",
    "#y_true = tumor[\"grade\"]\n",
    "#y_true = all[\"grade\"]\n",
    "y_true = vat6[\"grade\"]\n",
    "\n",
    "#y_probs=[tumor[\"Probability1\"],tumor[\"Probability2\"],tumor[\"Probability3\"],tumor[\"Probability4\"],tumor[\"Probability5\"],tumor[\"meanpro\"]]\n",
    "#y_probs=[all[\"Probability1\"],all[\"Probability2\"],all[\"Probability3\"],all[\"Probability4\"],all[\"Probability5\"],all[\"meanpro\"]]\n",
    "y_probs=[vat6[\"Probability1\"],vat6[\"Probability2\"],vat6[\"Probability3\"],vat6[\"Probability4\"],vat6[\"Probability5\"],vat6[\"meanpro\"]]\n",
    "model_names = [\"Fold1\", \"Fold2\", \"Fold3\", \"Fold4\", \"Fold5\", \"Ensemble model\"]\n",
    "\n",
    "# Create a single subplot for all calibration curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Calculate Brier Score for each model and add it to the label\n",
    "for i, y_prob in enumerate(y_probs):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)\n",
    "    brier_score = brier_score_loss(y_true, y_prob)\n",
    "    label = f\"{model_names[i]} (Brier Score = {brier_score:.3f})\"\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label=label)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('Mean Predicted Probability', fontsize=12)\n",
    "plt.ylabel('Fraction of Positives', fontsize=12)\n",
    "plt.plot([0, 1], [0, 1], '-', lw=1, color=\"navy\")\n",
    "#plt.title('Calibration Curves for Different Tumor Models', fontsize=16)\n",
    "plt.legend(loc=\"upper left\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save the figure as a TIFF file\n",
    "#plt.savefig('tumor_calibration_curves.tiff', dpi=300, format='tiff', bbox_inches='tight')\n",
    "plt.savefig('vat6_calibration_curves.tiff', dpi=300, format='tiff', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# Assuming you have defined y_true1, y_prob1, y_true2, y_prob2, ..., y_true6, y_prob6\n",
    "# y_trueX should contain the true labels, and y_probX should contain the predicted probabilities for each model and dataset\n",
    "\n",
    "#y_true = tumor[\"grade\"]\n",
    "#y_true = all[\"grade\"]\n",
    "y_true = vat6_1[\"grade\"]\n",
    "\n",
    "#y_probs=[tumor[\"Probability1\"],tumor[\"Probability2\"],tumor[\"Probability3\"],tumor[\"Probability4\"],tumor[\"Probability5\"],tumor[\"meanpro\"]]\n",
    "#y_probs=[all[\"Probability1\"],all[\"Probability2\"],all[\"Probability3\"],all[\"Probability4\"],all[\"Probability5\"],all[\"meanpro\"]]\n",
    "y_probs=[vat6_1[\"Probability1\"],vat6_1[\"Probability2\"],vat6_1[\"Probability3\"],vat6_1[\"Probability4\"],vat6_1[\"Probability5\"],vat6_1[\"meanpro\"]]\n",
    "model_names = [\"Fold1\", \"Fold2\", \"Fold3\", \"Fold4\", \"Fold5\", \"Ensemble model\"]\n",
    "\n",
    "# Create a single subplot for all calibration curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Calculate Brier Score for each model and add it to the label\n",
    "for i, y_prob in enumerate(y_probs):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)\n",
    "    brier_score = brier_score_loss(y_true, y_prob)\n",
    "    label = f\"{model_names[i]} (Brier Score = {brier_score:.3f})\"\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label=label)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('Mean Predicted Probability', fontsize=12)\n",
    "plt.ylabel('Fraction of Positives', fontsize=12)\n",
    "plt.plot([0, 1], [0, 1], '-', lw=1, color=\"navy\")\n",
    "#plt.title('Calibration Curves for Different Tumor Models', fontsize=16)\n",
    "plt.legend(loc=\"upper left\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save the figure as a TIFF file\n",
    "#plt.savefig('tumor_calibration_curves.tiff', dpi=300, format='tiff', bbox_inches='tight')\n",
    "plt.savefig('vat6_calibration_curves_KITS.tiff', dpi=300, format='tiff', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Assuming you have defined fpr1, tpr1, auc1, fpr2, tpr2, auc2, ..., fpr6, tpr6, auc6\n",
    "\n",
    "fpr1, tpr1, _ = roc_curve(tumor[\"grade\"], tumor[\"meanpro\"])\n",
    "fpr2, tpr2, _ = roc_curve(vat6[\"grade\"], vat6[\"meanpro\"])\n",
    "fpr3, tpr3, _ = roc_curve(all[\"grade\"], all[\"meanpro\"])\n",
    "fpr4, tpr4, _ = roc_curve(tumor_1[\"grade\"], tumor_1[\"meanpro\"])\n",
    "fpr5, tpr5, _ = roc_curve(vat6_1[\"grade\"], vat6_1[\"meanpro\"])\n",
    "fpr6, tpr6, _ = roc_curve(all_1[\"grade\"], all_1[\"meanpro\"])\n",
    "\n",
    "# Calculate the AUC scores\n",
    "auc1 = roc_auc_score(tumor[\"grade\"], tumor[\"meanpro\"])\n",
    "auc2 = roc_auc_score(vat6[\"grade\"], vat6[\"meanpro\"])\n",
    "auc3 = roc_auc_score(all[\"grade\"], all[\"meanpro\"])\n",
    "auc4 = roc_auc_score(tumor_1[\"grade\"], tumor_1[\"meanpro\"])\n",
    "auc5 = roc_auc_score(vat6_1[\"grade\"], vat6_1[\"meanpro\"])\n",
    "auc6 = roc_auc_score(all_1[\"grade\"], all_1[\"meanpro\"])\n",
    "\n",
    "fpr_list=[fpr1,fpr2,fpr3,fpr4,fpr5,fpr6]\n",
    "tpr_list=[tpr1,tpr2,tpr3,tpr4,tpr5,tpr6]\n",
    "auc_list=[auc1,auc2,auc3,auc4,auc5,auc6]\n",
    "\n",
    "cmap = cm.get_cmap('tab10')  # Choose a colormap\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "\n",
    "names=[\"Tumor: Validation\",\"VAT6: Validation\",\"Combination: Validation\",\"Tumor: External test\",\"VAT6: External test\",\"Combination: External test\"]\n",
    "\n",
    "# Creating subplots for Validation\n",
    "num_folds = 3\n",
    "#plt.subplot(1, 2, 1)\n",
    "label_names=[\"Ensemble model: Tumor\",\"Ensemble model: PRAT\",\"Ensemble model: Combination\"]\n",
    "for i in range(num_folds):\n",
    "    fold_label = '{}'.format(names[i])\n",
    "    plt.plot(fpr_list[i], tpr_list[i],\n",
    "             label=label_names[i]+' ' +'(AUC = {:.3f})'.format(auc_list[i]),\n",
    "             color=cmap(i), linestyle='-', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "#plt.title('ROC Curves of Internal Validation', fontsize=16)\n",
    "plt.legend(loc=\"lower right\",fontsize=12)\n",
    "\n",
    "plt.savefig('ROC_validation.tiff', dpi=300, format='tiff', bbox_inches='tight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "# Creating subplots for Test\n",
    "#plt.subplot(1, 2, 2)\n",
    "label_names=[\"Ensemble model: Tumor\",\"Ensemble model: PRAT\",\"Ensemble model: Combination\"]\n",
    "for i in range(num_folds, num_folds * 2):\n",
    "    fold_label = '{}'.format(names[i])\n",
    "    plt.plot(fpr_list[i], tpr_list[i],\n",
    "             label=label_names[i-3]+' '+'(AUC = {:.3f})'.format(auc_list[i]),\n",
    "             color=cmap(i-3), linestyle='-', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "#plt.title('Test ROC Curves', fontsize=16)\n",
    "plt.legend(loc=\"lower right\",fontsize=12)\n",
    "plt.savefig('ROC_KITS.tiff', dpi=300, format='tiff', bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Assuming you have defined fpr1, tpr1, auc1, fpr2, tpr2, auc2, ..., fpr6, tpr6, auc6\n",
    "\n",
    "fpr1, tpr1, _ = roc_curve(tumor[\"grade\"], tumor[\"meanpro\"])\n",
    "fpr2, tpr2, _ = roc_curve(vat6[\"grade\"], vat6[\"meanpro\"])\n",
    "fpr3, tpr3, _ = roc_curve(all[\"grade\"], all[\"meanpro\"])\n",
    "fpr4, tpr4, _ = roc_curve(tumor_1[\"grade\"], tumor_1[\"meanpro\"])\n",
    "fpr5, tpr5, _ = roc_curve(vat6_1[\"grade\"], vat6_1[\"meanpro\"])\n",
    "fpr6, tpr6, _ = roc_curve(all_1[\"grade\"], all_1[\"meanpro\"])\n",
    "\n",
    "\n",
    "# Calculate the AUC scores\n",
    "auc1 = roc_auc_score(tumor[\"grade\"], tumor[\"meanpro\"])\n",
    "auc2 = roc_auc_score(vat6[\"grade\"], vat6[\"meanpro\"])\n",
    "auc3 = roc_auc_score(all[\"grade\"], all[\"meanpro\"])\n",
    "auc4 = roc_auc_score(tumor_1[\"grade\"], tumor_1[\"meanpro\"])\n",
    "auc5 = roc_auc_score(vat6_1[\"grade\"], vat6_1[\"meanpro\"])\n",
    "auc6 = roc_auc_score(all_1[\"grade\"], all_1[\"meanpro\"])\n",
    "\n",
    "fpr_list=[fpr1,fpr2,fpr3,fpr4,fpr5,fpr6]\n",
    "tpr_list=[tpr1,tpr2,tpr3,tpr4,tpr5,tpr6]\n",
    "auc_list=[auc1,auc2,auc3,auc4,auc5,auc6]\n",
    "\n",
    "cmap = cm.get_cmap('tab10')  # Choose a colormap\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "names=[\"Tumor: Validation\",\"VAT6: Validation\",\"Combination: Validation\",\"Tumor: External test\",\"VAT6: External test\",\"Combination: External test\"]\n",
    "\n",
    "# Creating subplots\n",
    "num_folds = 6\n",
    "for i in range(num_folds):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    fold_label = '{}'.format(names[i])\n",
    "    plt.plot(fpr_list[i], tpr_list[i],\n",
    "             label='Ensemble model (area = {:.3f})'.format(auc_list[i]),\n",
    "             color=cmap(i), linestyle='-', linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('{}'.format(fold_label), fontsize=16)\n",
    "    plt.legend(loc=\"lower right\",fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.suptitle('ROC Curves for Folds', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Assuming you have the necessary data for auc1, auc2, auc3\n",
    "\n",
    "# Calculate the ROC curves\n",
    "fpr1, tpr1, _ = roc_curve(tumor[\"grade\"], tumor[\"Probability1\"])\n",
    "fpr2, tpr2, _ = roc_curve(tumor[\"grade\"], tumor[\"Probability2\"])\n",
    "fpr3, tpr3, _ = roc_curve(tumor[\"grade\"], tumor[\"Probability3\"])\n",
    "fpr4, tpr4, _ = roc_curve(tumor[\"grade\"], tumor[\"Probability4\"])\n",
    "fpr5, tpr5, _ = roc_curve(tumor[\"grade\"], tumor[\"Probability5\"])\n",
    "fpr6, tpr6, _ = roc_curve(tumor[\"grade\"], tumor[\"meanpro\"])\n",
    "\n",
    "\n",
    "# Calculate the AUC scores\n",
    "auc1 = roc_auc_score(tumor[\"grade\"], tumor[\"Probability1\"])\n",
    "auc2 = roc_auc_score(tumor[\"grade\"], tumor[\"Probability2\"])\n",
    "auc3 = roc_auc_score(tumor[\"grade\"], tumor[\"Probability3\"])\n",
    "auc4 = roc_auc_score(tumor[\"grade\"], tumor[\"Probability4\"])\n",
    "auc5 = roc_auc_score(tumor[\"grade\"], tumor[\"Probability5\"])\n",
    "auc6 = roc_auc_score(tumor[\"grade\"], tumor[\"meanpro\"])\n",
    "\n",
    "fpr_list=[fpr1,fpr2,fpr3,fpr4,fpr5,fpr6]\n",
    "tpr_list=[tpr1,tpr2,tpr3,tpr4,tpr5,tpr6]\n",
    "auc_list=[auc1,auc2,auc3,auc4,auc5,auc6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Assuming you have defined fpr1, tpr1, auc1, fpr2, tpr2, auc2, ..., fpr6, tpr6, auc6\n",
    "\n",
    "cmap = cm.get_cmap('tab10')  # Choose a colormap\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot each fold's ROC curve\n",
    "for i in range(1, 6):  # Assuming you have 6 folds\n",
    "    fpr = globals()['fpr{}'.format(i)]  # Get fpr for fold i\n",
    "    tpr = globals()['tpr{}'.format(i)]  # Get tpr for fold i\n",
    "    auc = globals()['auc{}'.format(i)]  # Get auc for fold i\n",
    "    fold_label = 'Fold{}'.format(i)\n",
    "    plt.plot(fpr, tpr,\n",
    "             label='{} (area = {:.2f})'.format(fold_label, auc),\n",
    "             color=cmap(i * 2), linestyle='dashed', linewidth=1)\n",
    "#'-', '--', '-.', ':', 'None', ' ', '', 'solid', 'dashed', 'dashdot', 'dotted'\n",
    "# Plot ensemble ROC curve\n",
    "\n",
    "plt.plot(fpr6, tpr6,\n",
    "         label='Ensemble (area = {:.2f})'.format(auc6),\n",
    "         color=cmap(7), linestyle='-', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Tumor: ROC Curves for validation set')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colormap\n",
    "cmap = LinearSegmentedColormap.from_list('roc_colormap', ['#FFA07A', '#008B8B'], N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "predictions = (tumor[\"meanpro\"] >= threshold).astype(int)\n",
    "\n",
    "pr_auc = average_precision_score(tumor[\"grade\"], tumor[\"meanpro\"])\n",
    "\n",
    "\n",
    "\n",
    "pre = precision_score(tumor[\"grade\"], predictions)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc = accuracy_score(tumor[\"grade\"], predictions)\n",
    "\n",
    "# Calculate recall\n",
    "rec = recall_score(tumor[\"grade\"], predictions)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(tumor[\"grade\"], predictions)\n",
    "\n",
    "print(\"PR-AUC:\", pr_auc)\n",
    "print(\"Precision:\", pre)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
